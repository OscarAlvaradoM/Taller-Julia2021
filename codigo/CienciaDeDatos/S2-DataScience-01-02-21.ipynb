{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "delayed-publication",
   "metadata": {},
   "source": [
    "# Segundo notebook de ciencia de datos\n",
    "- Óscar Alvarado\n",
    "- Oscar Esquivel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-nelson",
   "metadata": {},
   "source": [
    "# ScikitLearn.jl\n",
    "[sklearn](https://scikitlearnjl.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "virgin-trunk",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling RDatasets [ce6b1742-4840-55fa-b093-852dadbb1d8b]\n",
      "└ @ Base loading.jl:1278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "150-element Array{String,1}:\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " \"setosa\"\n",
       " ⋮\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\"\n",
       " \"virginica\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using RDatasets: dataset\n",
    "\n",
    "iris = dataset(\"datasets\", \"iris\")\n",
    "\n",
    "# ScikitLearn.jl espera arreglos pero también podemos utilizar DataFrames\n",
    "X = convert(Array, iris[[:SepalLength, :SepalWidth, :PetalLength, :PetalWidth]])\n",
    "y = convert(Array, iris[:Species])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "contrary-operation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>SepalLength</th><th>SepalWidth</th><th>PetalLength</th><th>PetalWidth</th><th>Species</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Cat…</th></tr></thead><tbody><p>150 rows × 5 columns</p><tr><th>1</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>setosa</td></tr><tr><th>2</th><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>setosa</td></tr><tr><th>3</th><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>setosa</td></tr><tr><th>4</th><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>setosa</td></tr><tr><th>5</th><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>setosa</td></tr><tr><th>6</th><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>setosa</td></tr><tr><th>7</th><td>4.6</td><td>3.4</td><td>1.4</td><td>0.3</td><td>setosa</td></tr><tr><th>8</th><td>5.0</td><td>3.4</td><td>1.5</td><td>0.2</td><td>setosa</td></tr><tr><th>9</th><td>4.4</td><td>2.9</td><td>1.4</td><td>0.2</td><td>setosa</td></tr><tr><th>10</th><td>4.9</td><td>3.1</td><td>1.5</td><td>0.1</td><td>setosa</td></tr><tr><th>11</th><td>5.4</td><td>3.7</td><td>1.5</td><td>0.2</td><td>setosa</td></tr><tr><th>12</th><td>4.8</td><td>3.4</td><td>1.6</td><td>0.2</td><td>setosa</td></tr><tr><th>13</th><td>4.8</td><td>3.0</td><td>1.4</td><td>0.1</td><td>setosa</td></tr><tr><th>14</th><td>4.3</td><td>3.0</td><td>1.1</td><td>0.1</td><td>setosa</td></tr><tr><th>15</th><td>5.8</td><td>4.0</td><td>1.2</td><td>0.2</td><td>setosa</td></tr><tr><th>16</th><td>5.7</td><td>4.4</td><td>1.5</td><td>0.4</td><td>setosa</td></tr><tr><th>17</th><td>5.4</td><td>3.9</td><td>1.3</td><td>0.4</td><td>setosa</td></tr><tr><th>18</th><td>5.1</td><td>3.5</td><td>1.4</td><td>0.3</td><td>setosa</td></tr><tr><th>19</th><td>5.7</td><td>3.8</td><td>1.7</td><td>0.3</td><td>setosa</td></tr><tr><th>20</th><td>5.1</td><td>3.8</td><td>1.5</td><td>0.3</td><td>setosa</td></tr><tr><th>21</th><td>5.4</td><td>3.4</td><td>1.7</td><td>0.2</td><td>setosa</td></tr><tr><th>22</th><td>5.1</td><td>3.7</td><td>1.5</td><td>0.4</td><td>setosa</td></tr><tr><th>23</th><td>4.6</td><td>3.6</td><td>1.0</td><td>0.2</td><td>setosa</td></tr><tr><th>24</th><td>5.1</td><td>3.3</td><td>1.7</td><td>0.5</td><td>setosa</td></tr><tr><th>25</th><td>4.8</td><td>3.4</td><td>1.9</td><td>0.2</td><td>setosa</td></tr><tr><th>26</th><td>5.0</td><td>3.0</td><td>1.6</td><td>0.2</td><td>setosa</td></tr><tr><th>27</th><td>5.0</td><td>3.4</td><td>1.6</td><td>0.4</td><td>setosa</td></tr><tr><th>28</th><td>5.2</td><td>3.5</td><td>1.5</td><td>0.2</td><td>setosa</td></tr><tr><th>29</th><td>5.2</td><td>3.4</td><td>1.4</td><td>0.2</td><td>setosa</td></tr><tr><th>30</th><td>4.7</td><td>3.2</td><td>1.6</td><td>0.2</td><td>setosa</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& SepalLength & SepalWidth & PetalLength & PetalWidth & Species\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Cat…\\\\\n",
       "\t\\hline\n",
       "\t1 & 5.1 & 3.5 & 1.4 & 0.2 & setosa \\\\\n",
       "\t2 & 4.9 & 3.0 & 1.4 & 0.2 & setosa \\\\\n",
       "\t3 & 4.7 & 3.2 & 1.3 & 0.2 & setosa \\\\\n",
       "\t4 & 4.6 & 3.1 & 1.5 & 0.2 & setosa \\\\\n",
       "\t5 & 5.0 & 3.6 & 1.4 & 0.2 & setosa \\\\\n",
       "\t6 & 5.4 & 3.9 & 1.7 & 0.4 & setosa \\\\\n",
       "\t7 & 4.6 & 3.4 & 1.4 & 0.3 & setosa \\\\\n",
       "\t8 & 5.0 & 3.4 & 1.5 & 0.2 & setosa \\\\\n",
       "\t9 & 4.4 & 2.9 & 1.4 & 0.2 & setosa \\\\\n",
       "\t10 & 4.9 & 3.1 & 1.5 & 0.1 & setosa \\\\\n",
       "\t11 & 5.4 & 3.7 & 1.5 & 0.2 & setosa \\\\\n",
       "\t12 & 4.8 & 3.4 & 1.6 & 0.2 & setosa \\\\\n",
       "\t13 & 4.8 & 3.0 & 1.4 & 0.1 & setosa \\\\\n",
       "\t14 & 4.3 & 3.0 & 1.1 & 0.1 & setosa \\\\\n",
       "\t15 & 5.8 & 4.0 & 1.2 & 0.2 & setosa \\\\\n",
       "\t16 & 5.7 & 4.4 & 1.5 & 0.4 & setosa \\\\\n",
       "\t17 & 5.4 & 3.9 & 1.3 & 0.4 & setosa \\\\\n",
       "\t18 & 5.1 & 3.5 & 1.4 & 0.3 & setosa \\\\\n",
       "\t19 & 5.7 & 3.8 & 1.7 & 0.3 & setosa \\\\\n",
       "\t20 & 5.1 & 3.8 & 1.5 & 0.3 & setosa \\\\\n",
       "\t21 & 5.4 & 3.4 & 1.7 & 0.2 & setosa \\\\\n",
       "\t22 & 5.1 & 3.7 & 1.5 & 0.4 & setosa \\\\\n",
       "\t23 & 4.6 & 3.6 & 1.0 & 0.2 & setosa \\\\\n",
       "\t24 & 5.1 & 3.3 & 1.7 & 0.5 & setosa \\\\\n",
       "\t25 & 4.8 & 3.4 & 1.9 & 0.2 & setosa \\\\\n",
       "\t26 & 5.0 & 3.0 & 1.6 & 0.2 & setosa \\\\\n",
       "\t27 & 5.0 & 3.4 & 1.6 & 0.4 & setosa \\\\\n",
       "\t28 & 5.2 & 3.5 & 1.5 & 0.2 & setosa \\\\\n",
       "\t29 & 5.2 & 3.4 & 1.4 & 0.2 & setosa \\\\\n",
       "\t30 & 4.7 & 3.2 & 1.6 & 0.2 & setosa \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "150×5 DataFrame\n",
       "│ Row │ SepalLength │ SepalWidth │ PetalLength │ PetalWidth │ Species   │\n",
       "│     │ \u001b[90mFloat64\u001b[39m     │ \u001b[90mFloat64\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │ \u001b[90mFloat64\u001b[39m    │ \u001b[90mCat…\u001b[39m      │\n",
       "├─────┼─────────────┼────────────┼─────────────┼────────────┼───────────┤\n",
       "│ 1   │ 5.1         │ 3.5        │ 1.4         │ 0.2        │ setosa    │\n",
       "│ 2   │ 4.9         │ 3.0        │ 1.4         │ 0.2        │ setosa    │\n",
       "│ 3   │ 4.7         │ 3.2        │ 1.3         │ 0.2        │ setosa    │\n",
       "│ 4   │ 4.6         │ 3.1        │ 1.5         │ 0.2        │ setosa    │\n",
       "│ 5   │ 5.0         │ 3.6        │ 1.4         │ 0.2        │ setosa    │\n",
       "│ 6   │ 5.4         │ 3.9        │ 1.7         │ 0.4        │ setosa    │\n",
       "│ 7   │ 4.6         │ 3.4        │ 1.4         │ 0.3        │ setosa    │\n",
       "│ 8   │ 5.0         │ 3.4        │ 1.5         │ 0.2        │ setosa    │\n",
       "│ 9   │ 4.4         │ 2.9        │ 1.4         │ 0.2        │ setosa    │\n",
       "│ 10  │ 4.9         │ 3.1        │ 1.5         │ 0.1        │ setosa    │\n",
       "⋮\n",
       "│ 140 │ 6.9         │ 3.1        │ 5.4         │ 2.1        │ virginica │\n",
       "│ 141 │ 6.7         │ 3.1        │ 5.6         │ 2.4        │ virginica │\n",
       "│ 142 │ 6.9         │ 3.1        │ 5.1         │ 2.3        │ virginica │\n",
       "│ 143 │ 5.8         │ 2.7        │ 5.1         │ 1.9        │ virginica │\n",
       "│ 144 │ 6.8         │ 3.2        │ 5.9         │ 2.3        │ virginica │\n",
       "│ 145 │ 6.7         │ 3.3        │ 5.7         │ 2.5        │ virginica │\n",
       "│ 146 │ 6.7         │ 3.0        │ 5.2         │ 2.3        │ virginica │\n",
       "│ 147 │ 6.3         │ 2.5        │ 5.0         │ 1.9        │ virginica │\n",
       "│ 148 │ 6.5         │ 3.0        │ 5.2         │ 2.0        │ virginica │\n",
       "│ 149 │ 6.2         │ 3.4        │ 5.4         │ 2.3        │ virginica │\n",
       "│ 150 │ 5.9         │ 3.0        │ 5.1         │ 1.8        │ virginica │"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "announced-representative",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.linear_model._logistic.LogisticRegression'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import Pkg; Pkg.add(\"ScikitLearn\")\n",
    "using ScikitLearn\n",
    "\n",
    "@sk_import linear_model: LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "chemical-couple",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(fit_intercept=true, max_iter = 200)\n",
    "\n",
    "fit!(model, X, y)\n",
    "\n",
    "accuracy = sum(predict(model, X) .== y) / length(y)\n",
    "println(\"accuracy: $accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "stuck-drama",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 0.9666666666666667\n",
       " 1.0\n",
       " 0.9333333333333333\n",
       " 0.9666666666666667\n",
       " 1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ScikitLearn.CrossValidation: cross_val_score\n",
    "\n",
    "cross_val_score(LogisticRegression(max_iter = 200), X, y; cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "known-russell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mg\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mR\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mg\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "    Logistic Regression (aka logit, MaxEnt) classifier.\n",
       "\n",
       "    In the multiclass case, the training algorithm uses the one-vs-rest (OvR)\n",
       "    scheme if the 'multi_class' option is set to 'ovr', and uses the\n",
       "    cross-entropy loss if the 'multi_class' option is set to 'multinomial'.\n",
       "    (Currently the 'multinomial' option is supported only by the 'lbfgs',\n",
       "    'sag', 'saga' and 'newton-cg' solvers.)\n",
       "\n",
       "    This class implements regularized logistic regression using the\n",
       "    'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n",
       "    that regularization is applied by default**. It can handle both dense\n",
       "    and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n",
       "    floats for optimal performance; any other input format will be converted\n",
       "    (and copied).\n",
       "\n",
       "    The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n",
       "    with primal formulation, or no regularization. The 'liblinear' solver\n",
       "    supports both L1 and L2 regularization, with a dual formulation only for\n",
       "    the L2 penalty. The Elastic-Net regularization is only supported by the\n",
       "    'saga' solver.\n",
       "\n",
       "    Read more in the :ref:`User Guide <logistic_regression>`.\n",
       "\n",
       "    Parameters\n",
       "    ----------\n",
       "    penalty : {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n",
       "        Used to specify the norm used in the penalization. The 'newton-cg',\n",
       "        'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n",
       "        only supported by the 'saga' solver. If 'none' (not supported by the\n",
       "        liblinear solver), no regularization is applied.\n",
       "\n",
       "        .. versionadded:: 0.19\n",
       "           l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n",
       "\n",
       "    dual : bool, default=False\n",
       "        Dual or primal formulation. Dual formulation is only implemented for\n",
       "        l2 penalty with liblinear solver. Prefer dual=False when\n",
       "        n_samples > n_features.\n",
       "\n",
       "    tol : float, default=1e-4\n",
       "        Tolerance for stopping criteria.\n",
       "\n",
       "    C : float, default=1.0\n",
       "        Inverse of regularization strength; must be a positive float.\n",
       "        Like in support vector machines, smaller values specify stronger\n",
       "        regularization.\n",
       "\n",
       "    fit_intercept : bool, default=True\n",
       "        Specifies if a constant (a.k.a. bias or intercept) should be\n",
       "        added to the decision function.\n",
       "\n",
       "    intercept_scaling : float, default=1\n",
       "        Useful only when the solver 'liblinear' is used\n",
       "        and self.fit_intercept is set to True. In this case, x becomes\n",
       "        [x, self.intercept_scaling],\n",
       "        i.e. a \"synthetic\" feature with constant value equal to\n",
       "        intercept_scaling is appended to the instance vector.\n",
       "        The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n",
       "\n",
       "        Note! the synthetic feature weight is subject to l1/l2 regularization\n",
       "        as all other features.\n",
       "        To lessen the effect of regularization on synthetic feature weight\n",
       "        (and therefore on the intercept) intercept_scaling has to be increased.\n",
       "\n",
       "    class_weight : dict or 'balanced', default=None\n",
       "        Weights associated with classes in the form ``{class_label: weight}``.\n",
       "        If not given, all classes are supposed to have weight one.\n",
       "\n",
       "        The \"balanced\" mode uses the values of y to automatically adjust\n",
       "        weights inversely proportional to class frequencies in the input data\n",
       "        as ``n_samples / (n_classes * np.bincount(y))``.\n",
       "\n",
       "        Note that these weights will be multiplied with sample_weight (passed\n",
       "        through the fit method) if sample_weight is specified.\n",
       "\n",
       "        .. versionadded:: 0.17\n",
       "           *class_weight='balanced'*\n",
       "\n",
       "    random_state : int, RandomState instance, default=None\n",
       "        Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n",
       "        data. See :term:`Glossary <random_state>` for details.\n",
       "\n",
       "    solver : {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'},             default='lbfgs'\n",
       "\n",
       "        Algorithm to use in the optimization problem.\n",
       "\n",
       "        - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n",
       "          'saga' are faster for large ones.\n",
       "        - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n",
       "          handle multinomial loss; 'liblinear' is limited to one-versus-rest\n",
       "          schemes.\n",
       "        - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n",
       "        - 'liblinear' and 'saga' also handle L1 penalty\n",
       "        - 'saga' also supports 'elasticnet' penalty\n",
       "        - 'liblinear' does not support setting ``penalty='none'``\n",
       "\n",
       "        Note that 'sag' and 'saga' fast convergence is only guaranteed on\n",
       "        features with approximately the same scale. You can\n",
       "        preprocess the data with a scaler from sklearn.preprocessing.\n",
       "\n",
       "        .. versionadded:: 0.17\n",
       "           Stochastic Average Gradient descent solver.\n",
       "        .. versionadded:: 0.19\n",
       "           SAGA solver.\n",
       "        .. versionchanged:: 0.22\n",
       "            The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n",
       "\n",
       "    max_iter : int, default=100\n",
       "        Maximum number of iterations taken for the solvers to converge.\n",
       "\n",
       "    multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n",
       "        If the option chosen is 'ovr', then a binary problem is fit for each\n",
       "        label. For 'multinomial' the loss minimised is the multinomial loss fit\n",
       "        across the entire probability distribution, *even when the data is\n",
       "        binary*. 'multinomial' is unavailable when solver='liblinear'.\n",
       "        'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n",
       "        and otherwise selects 'multinomial'.\n",
       "\n",
       "        .. versionadded:: 0.18\n",
       "           Stochastic Average Gradient descent solver for 'multinomial' case.\n",
       "        .. versionchanged:: 0.22\n",
       "            Default changed from 'ovr' to 'auto' in 0.22.\n",
       "\n",
       "    verbose : int, default=0\n",
       "        For the liblinear and lbfgs solvers set verbose to any positive\n",
       "        number for verbosity.\n",
       "\n",
       "    warm_start : bool, default=False\n",
       "        When set to True, reuse the solution of the previous call to fit as\n",
       "        initialization, otherwise, just erase the previous solution.\n",
       "        Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n",
       "\n",
       "        .. versionadded:: 0.17\n",
       "           *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n",
       "\n",
       "    n_jobs : int, default=None\n",
       "        Number of CPU cores used when parallelizing over classes if\n",
       "        multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n",
       "        set to 'liblinear' regardless of whether 'multi_class' is specified or\n",
       "        not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
       "        context. ``-1`` means using all processors.\n",
       "        See :term:`Glossary <n_jobs>` for more details.\n",
       "\n",
       "    l1_ratio : float, default=None\n",
       "        The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n",
       "        used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n",
       "        to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n",
       "        to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n",
       "        combination of L1 and L2.\n",
       "\n",
       "    Attributes\n",
       "    ----------\n",
       "\n",
       "    classes_ : ndarray of shape (n_classes, )\n",
       "        A list of class labels known to the classifier.\n",
       "\n",
       "    coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n",
       "        Coefficient of the features in the decision function.\n",
       "\n",
       "        `coef_` is of shape (1, n_features) when the given problem is binary.\n",
       "        In particular, when `multi_class='multinomial'`, `coef_` corresponds\n",
       "        to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n",
       "\n",
       "    intercept_ : ndarray of shape (1,) or (n_classes,)\n",
       "        Intercept (a.k.a. bias) added to the decision function.\n",
       "\n",
       "        If `fit_intercept` is set to False, the intercept is set to zero.\n",
       "        `intercept_` is of shape (1,) when the given problem is binary.\n",
       "        In particular, when `multi_class='multinomial'`, `intercept_`\n",
       "        corresponds to outcome 1 (True) and `-intercept_` corresponds to\n",
       "        outcome 0 (False).\n",
       "\n",
       "    n_iter_ : ndarray of shape (n_classes,) or (1, )\n",
       "        Actual number of iterations for all classes. If binary or multinomial,\n",
       "        it returns only 1 element. For liblinear solver, only the maximum\n",
       "        number of iteration across all classes is given.\n",
       "\n",
       "        .. versionchanged:: 0.20\n",
       "\n",
       "            In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n",
       "            ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n",
       "\n",
       "    See Also\n",
       "    --------\n",
       "    SGDClassifier : Incrementally trained logistic regression (when given\n",
       "        the parameter ``loss=\"log\"``).\n",
       "    LogisticRegressionCV : Logistic regression with built-in cross validation.\n",
       "\n",
       "    Notes\n",
       "    -----\n",
       "    The underlying C implementation uses a random number generator to\n",
       "    select features when fitting the model. It is thus not uncommon,\n",
       "    to have slightly different results for the same input data. If\n",
       "    that happens, try with a smaller tol parameter.\n",
       "\n",
       "    Predict output may not match that of standalone liblinear in certain\n",
       "    cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
       "    in the narrative documentation.\n",
       "\n",
       "    References\n",
       "    ----------\n",
       "\n",
       "    L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n",
       "        Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n",
       "        http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n",
       "\n",
       "    LIBLINEAR -- A Library for Large Linear Classification\n",
       "        https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n",
       "\n",
       "    SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n",
       "        Minimizing Finite Sums with the Stochastic Average Gradient\n",
       "        https://hal.inria.fr/hal-00860051/document\n",
       "\n",
       "    SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n",
       "        SAGA: A Fast Incremental Gradient Method With Support\n",
       "        for Non-Strongly Convex Composite Objectives\n",
       "        https://arxiv.org/abs/1407.0202\n",
       "\n",
       "    Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n",
       "        methods for logistic regression and maximum entropy models.\n",
       "        Machine Learning 85(1-2):41-75.\n",
       "        https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n",
       "\n",
       "    Examples\n",
       "    --------\n",
       "    >>> from sklearn.datasets import load_iris\n",
       "    >>> from sklearn.linear_model import LogisticRegression\n",
       "    >>> X, y = load_iris(return_X_y=True)\n",
       "    >>> clf = LogisticRegression(random_state=0).fit(X, y)\n",
       "    >>> clf.predict(X[:2, :])\n",
       "    array([0, 0])\n",
       "    >>> clf.predict_proba(X[:2, :])\n",
       "    array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
       "           [9.7...e-01, 2.8...e-02, ...e-08]])\n",
       "    >>> clf.score(X, y)\n",
       "    0.97...\n",
       "    "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "union-sunrise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor parámetro: Dict{Symbol,Any}(:C => 0.6)\n"
     ]
    }
   ],
   "source": [
    "using ScikitLearn.GridSearch: GridSearchCV\n",
    "\n",
    "gridsearch = GridSearchCV(LogisticRegression(max_iter = 200), Dict(:C => 0.1:0.1:2.0))\n",
    "fit!(gridsearch, X, y)\n",
    "println(\"Mejor parámetro: $(gridsearch.best_params_)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-starter",
   "metadata": {},
   "source": [
    "## Flux.jl\n",
    "[Flux](https://fluxml.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "elect-turkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux, Flux.Data.MNIST, Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: repeated, partition\n",
    "using Printf, BSON, BenchmarkTools\n",
    "#using Pkg; Pkg.add([\"Netpbm\", \"Images\", \"ImageFeatures\"])\n",
    "using Images, ImageFeatures # We have to Pkg.add(\"Netpbm\") because our images are in pgm format\n",
    "#import Pkg; Pkg.add([\"ImageIO\", \"ImageMagick\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "patient-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "scenic-brighton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in the train set: (60000,)\n",
      "Images in the test set: (10000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QA/4ePzL8AAAIKSURBVGje7dpPiI1RGMfxzyAL8qfZmFISi1EiFigpSZJiMbGhbLBDVjZ2FqSwQBazUhayxYryd6FuTf5syN6fHYM0yGDxvJPr3tu9c2eKc0/nW29v5z3ve3/93qdz3uc551IoFAqFQqHQ+/R1+8BMLKhrH8EcDOIwzmEvvuIMTjY8P+NfO8xfcFanG5ZgNjZiExZid4v7XuMihvAZz/EwBYf5C7Ydh2tx19/jrhU/cQBfqvZbfMCrFBzmL9g2hv2oYVmLvhpGsQXfdY7zf3OYv2DbufQ9jmMnnoq5Ep5hmxh3K3EsZYf5C04qp5kvvnHDOIj9uNYrDvMX7JjTwKfq/LE6H8J18R1M3mH+gl3VFnNxC5uxA3d6wWH+gl3Xh8vxROQz9zGCy/iVqsP8BbuOIVEDXsG8qn0CV/EuRYf5C04phrAK57G1ag/jFN6k5jB/wSnHkFiz2SXGZB/uiZojKYf5C04rhhN8EwnuD2zHg5Qc5i84qdqiFauxB+vqfuQFHqXmMH/BrmM4iKMirxmouz4ucppONWP+rzTduXQA+8T+0tKGvhGRz9xM0WH+gh1juEisiV7Cioa+Gs7ihsmv2eT/StOJYb+oF9Zo3rd4LOqK2xhL3WH+gk0x3CD2KtZjcUPfGC7gtD97hck7zF+wKacZqo4JXoo10nHxP4vRXnOYv2ChUCgUps9vDE1MYMzifHwAAAAASUVORK5CYII=",
      "text/plain": [
       "28×28 Array{Gray{N0f8},2} with eltype Gray{Normed{UInt8,8}}:\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " ⋮                                 ⋱                   \n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)  …  Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)\n",
       " Gray{N0f8}(0.0)  Gray{N0f8}(0.0)     Gray{N0f8}(0.0)  Gray{N0f8}(0.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load labels and images from Flux.Data.MNIST\n",
    "# Train set: images used to estimate the CNN\n",
    "# Load data on gpu (if enabled)\n",
    "train_labels = MNIST.labels(:train);\n",
    "train_imgs = MNIST.images(:train);\n",
    "\n",
    "# Test set: images used to see how well the CNN perform \"out-of-the-sample\"\n",
    "test_imgs = MNIST.images(:test)\n",
    "test_labels = MNIST.labels(:test)\n",
    "\n",
    "println(\"Images in the train set: $(size(train_imgs))\")\n",
    "println(\"Images in the test set: $(size(test_imgs))\")\n",
    "\n",
    "# Visualization of one digit\n",
    "NROWS, NCOLS = 28, 28\n",
    "a = reshape(train_imgs[1], NROWS, NCOLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "drawn-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = MNIST.images(:train);\n",
    "labels = onehotbatch(MNIST.labels(:train), 0:9);\n",
    "\n",
    "# Partition into batches of size 1,000\n",
    "train = [(cat(float.(imgs[i])..., dims = 4), labels[:,i])\n",
    "         for i in partition(1:60_000, 1000)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "focal-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = cat(float.(MNIST.images(:test)[1:1000])..., dims = 4)\n",
    "tY = onehotbatch(MNIST.labels(:test)[1:1000], 0:9);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "anonymous-listing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Conv((3, 3), 1=>16, relu), MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2)), Conv((3, 3), 16=>32, relu), MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2)), Conv((3, 3), 32=>32, relu), MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2)), #3, Dense(288, 10), softmax)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model definition\n",
    "# See: https://github.com/FluxML/model-zoo/blob/master/vision/mnist/conv.jl\n",
    "model = Chain(\n",
    "    # First convolution, operating upon a 28x28 image\n",
    "    Conv((3, 3), 1=>16, pad=(1,1), relu),\n",
    "    MaxPool((2,2)), #maxpooling\n",
    "\n",
    "    # Second convolution, operating upon a 14x14 image\n",
    "    Conv((3, 3), 16=>32, pad=(1,1), relu),\n",
    "    MaxPool((2,2)), #maxpooling\n",
    "\n",
    "    # Third convolution, operating upon a 7x7 image\n",
    "    Conv((3, 3), 32=>32,pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Reshape 3d tensor into a 2d one, at this point it should be (3, 3, 32, N)\n",
    "    # which is where we get the 288 in the `Dense` layer below:\n",
    "    x -> reshape(x, :, size(x, 4)),\n",
    "    Dense(288, 10),\n",
    "\n",
    "    # Softmax to get probabilities\n",
    "    softmax,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "portuguese-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batching \n",
    "# See: https://github.com/FluxML/model-zoo/blob/master/vision/mnist/conv.jl\n",
    "# Bundle images together with labels and group into minibatchess\n",
    "function make_minibatch(X, Y, idxs)\n",
    "    X_batch = Array{Float32}(undef, size(X[1])..., 1, length(idxs))\n",
    "    for i in 1:length(idxs)\n",
    "        X_batch[:, :, :, i] = Float32.(X[idxs[i]])\n",
    "    end\n",
    "    Y_batch = onehotbatch(Y[idxs], 0:9)\n",
    "    return (X_batch, Y_batch)\n",
    "end\n",
    "# The CNN only \"sees\" 128 images at each training cycle:\n",
    "batch_size = 128\n",
    "mb_idxs = partition(1:length(train_imgs), batch_size)\n",
    "# train set in the form of batches\n",
    "train_set = [make_minibatch(train_imgs, train_labels, i) for i in mb_idxs];\n",
    "# train set in one-go: used to calculate accuracy with the train set\n",
    "train_set_full = make_minibatch(train_imgs, train_labels, 1:length(train_imgs));\n",
    "# test set: to check we do not overfit the train data:\n",
    "test_set = make_minibatch(test_imgs, test_labels, 1:length(test_imgs));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "minor-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "# See: https://github.com/FluxML/model-zoo/blob/master/vision/mnist/conv.jl\n",
    "# `loss()` calculates the crossentropy loss between our prediction `y_hat`\n",
    "function loss(x, y)\n",
    "    # Add some noise to the image\n",
    "    # we reduce the risk of overfitting the train sample by doing so:\n",
    "    x_aug = x .+ 0.1f0*randn(eltype(x), size(x))\n",
    "\n",
    "    y_hat = model(x_aug)\n",
    "    return crossentropy(y_hat, y)\n",
    "end\n",
    "accuracy(x, y) = mean(onecold(model(x)) .== onecold(y))\n",
    "\n",
    "# ADAM optimizer\n",
    "opt = ADAM(0.001);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "latter-offering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  38.728 s (2180716 allocations: 26.35 GiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: [1]: Train accuracy: 0.9871\n",
      "└ @ Main In[13]:14\n",
      "┌ Info: [1]: Test accuracy: 0.9859\n",
      "└ @ Main In[13]:18\n",
      "┌ Info:  -> Early-exiting: We reached our target accuracy of 97.0%\n",
      "└ @ Main In[13]:22\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# See: https://github.com/FluxML/model-zoo/blob/master/vision/mnist/conv.jl\n",
    "best_acc = 0.0\n",
    "last_improvement = 0\n",
    "accuracy_target = 0.97 #Set an accuracy target. When reached, we stop training.\n",
    "max_epochs = 100 #Maximum\n",
    "for epoch_idx in 1:100\n",
    "    global best_acc, last_improvement\n",
    "    # Train for a single epoch\n",
    "    @btime Flux.train!(loss, Flux.params(model), train_set, opt)\n",
    "\n",
    "    # Calculate accuracy:\n",
    "    acc = accuracy(train_set_full...)\n",
    "    @info(@sprintf(\"[%d]: Train accuracy: %.4f\", epoch_idx, acc))\n",
    "    \n",
    "    # Calculate accuracy:\n",
    "    acc = accuracy(test_set...)\n",
    "    @info(@sprintf(\"[%d]: Test accuracy: %.4f\", epoch_idx, acc))\n",
    "\n",
    "    # If our accuracy is good enough, quit out.\n",
    "    if acc >= accuracy_target\n",
    "        @info(\" -> Early-exiting: We reached our target accuracy of $(accuracy_target*100)%\")\n",
    "        break\n",
    "    end\n",
    "\n",
    "    if epoch_idx - last_improvement >= 10\n",
    "        @warn(\" -> We're calling this converged.\")\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "colored-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and convert data to Array: \n",
    "pred = model(test_set[1]); \n",
    "\n",
    "# Function to get the row index of the max value: \n",
    "f1(x) = getindex.(argmax(x, dims=1), 1) # Final predicted value is the one with the maximum probability: \n",
    "pred = f1(pred) .- 1; #minus 1, because the first digit is 0 (not 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "quarterly-attribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value = 7\n"
     ]
    }
   ],
   "source": [
    "println(\"Predicted value = $(pred[1])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "impaired-arctic",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-russell",
   "metadata": {},
   "source": [
    "# Usando la GPU!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dynamic-bidder",
   "metadata": {},
   "source": [
    "### Cargando nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coordinate-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Pkg; Pkg.add(\"CUDA\")\n",
    "using CUDA\n",
    "#CUDA.allowscalar(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prepared-prague",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Pkg; Pkg.add(\"Flux\")\n",
    "#import Pkg; Pkg.add(\"BSON\")\n",
    "using Flux, Flux.Data.MNIST, Statistics\n",
    "using Flux: onehotbatch, onecold, crossentropy, throttle\n",
    "using Base.Iterators: partition\n",
    "using Printf, BSON\n",
    "using BenchmarkTools\n",
    "\n",
    "imgs =  Flux.Data.MNIST.images(:train);\n",
    "labels = onehotbatch( Flux.Data.MNIST.labels(:train), 0:9);\n",
    "\n",
    "# Partition into batches of size 1,000\n",
    "train = [(cat(float.(imgs[i])..., dims = 4), labels[:,i])\n",
    "         for i in partition(1:60_000, 64)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "social-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = gpu.(train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "medium-frederick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test set (first 1,000 images)\n",
    "tX = cat(float.(Flux.Data.MNIST.images(:test)[1:500])..., dims = 4) |> gpu;\n",
    "tY = onehotbatch(Flux.Data.MNIST.labels(:test)[1:500], 0:9) |> gpu;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-boundary",
   "metadata": {},
   "source": [
    "### Haciendo el modelo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "colonial-peeing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Conv((3, 3), 1=>16, relu), MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2)), Conv((3, 3), 16=>32, relu), MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2)), Conv((3, 3), 32=>32, relu), MaxPool((2, 2), pad = (0, 0, 0, 0), stride = (2, 2)), #3, Dense(288, 10), softmax)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Chain(\n",
    "  # First convolution, operating upon a 192×192image\n",
    "    Conv((3, 3), 1=>16, pad=(1,1), relu),\n",
    "    MaxPool((2,2)), #maxpooling\n",
    "\n",
    "    # Second convolution, operating upon a 96×96 image\n",
    "    Conv((3, 3), 16=>32, pad=(1,1), relu),\n",
    "    MaxPool((2,2)), #maxpooling\n",
    "\n",
    "    # Third convolution, operating upon a 48×48 image\n",
    "    Conv((3, 3), 32=>32,pad=(1,1), relu),\n",
    "    MaxPool((2,2)),\n",
    "\n",
    "    # Reshape 3d tensor into a 2d one, at this point it should be (24, 24, 32, N)\n",
    "    # which is where we get the 18432 in the `Dense` layer below:\n",
    "    x -> reshape(x, :, size(x, 4)),\n",
    "    Dense(288, 10), softmax) |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sapphire-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "#= Loss function\n",
    "# See: https://github.com/FluxML/model-zoo/blob/master/vision/mnist/conv.jl\n",
    "# `loss()` calculates the crossentropy loss between our prediction `y_hat`\n",
    "loss(x, y) = crossentropy(model(x), y)\n",
    "accuracy1(x, y) = mean(onecold(model(x)) .== onecold(y))\n",
    "evalcb = throttle(() -> @show(accuracy1(tX, tY)), 10)\n",
    "\n",
    "# ADAM optimizer\n",
    "opt = ADAM(0.001);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distant-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss(x, y)\n",
    "    # Add some noise to the image\n",
    "    # we reduce the risk of overfitting the train sample by doing so:\n",
    "    x_aug = x .+ 0.1f0*gpu(randn(eltype(x), size(x)))\n",
    "\n",
    "    y_hat = model(x_aug)\n",
    "    return crossentropy(y_hat, y)\n",
    "end\n",
    "accuracy1(x, y) = mean(onecold(model(x)) .== onecold(y))\n",
    "evalcb = throttle(() -> @show(accuracy1(tX, tY)), 10)\n",
    "\n",
    "# ADAM optimizer\n",
    "opt = ADAM(0.001);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alike-cargo",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "  2.742 s (7180960 allocations: 697.83 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Performing scalar operations on GPU arrays: This is very slow, consider disallowing these operations with `allowscalar(false)`\n",
      "└ @ GPUArrays /home/oscar/.julia/packages/GPUArrays/WV76E/src/host/indexing.jl:43\n",
      "┌ Info: [1]: Test accuracy: 0.9920\n",
      "└ @ Main In[8]:20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_idx = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info:  -> Early-exiting: We reached our target accuracy of 97.0%\n",
      "└ @ Main In[8]:24\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "# See: https://github.com/FluxML/model-zoo/blob/master/vision/mnist/conv.jl\n",
    "best_acc = 0.0\n",
    "last_improvement = 0\n",
    "accuracy_target = 0.97  #Set an accuracy target. When reached, we stop training.\n",
    "max_epochs = 200 #Maximum\n",
    "for epoch_idx in 1:max_epochs\n",
    "    global best_acc, last_improvement\n",
    "    # Train for a single epoch\n",
    "    println(\"Epoch $(epoch_idx)\")\n",
    "    @btime Flux.train!(loss, Flux.params(model), train, opt) \n",
    "    #Flux.train!(loss, Flux.params(model), train, opt) \n",
    "\n",
    "    # Calculate accuracy:\n",
    "    #acc = accuracy1(train_set_full...)\n",
    "    #@info(@sprintf(\"[%d]: Train accuracy: %.4f\", epoch_idx, acc))\n",
    "\n",
    "    # Calculate accuracy:\n",
    "    acc = accuracy1(tX, tY)\n",
    "    @info(@sprintf(\"[%d]: Test accuracy: %.4f\", epoch_idx, acc))\n",
    "\n",
    "    # If our accuracy is good enough, quit out.\n",
    "    if acc >= accuracy_target\n",
    "        @info(\" -> Early-exiting: We reached our target accuracy of $(accuracy_target*100)%\")\n",
    "        @show epoch_idx\n",
    "        break\n",
    "    end\n",
    "\n",
    "    if epoch_idx - last_improvement >= max_epochs\n",
    "        @warn(\" -> We're calling this converged.\")\n",
    "        break\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "spectacular-eagle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value = 7\n"
     ]
    }
   ],
   "source": [
    "# Get predictions and convert data to Array: \n",
    "pred = model(tX); \n",
    "\n",
    "# Function to get the row index of the max value: \n",
    "f1(x) = getindex.(argmax(x, dims=1), 1) # Final predicted value is the one with the maximum probability: \n",
    "pred = f1(pred) .- 1; #minus 1, because the first digit is 0 (not 1)\n",
    "\n",
    "println(\"Predicted value = $(pred[1])\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dependent-module",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tY[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-reminder",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
